{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"ImagingFormats/","title":"Medical Image Formats","text":""},{"location":"ImagingFormats/#introduction","title":"Introduction","text":"<p>A medical image is a digital representation of the internal structure or function of an anatomic region, typically presented as an array of picture elements called pixels (2D) or voxels (3D). This representation is a discrete mapping of numerical values to positions in space.</p>"},{"location":"ImagingFormats/#medical-image-metadata","title":"Medical Image Metadata","text":"<p>Medical images often come with metadata, which provides additional information about the image. This metadata is usually stored at the beginning of the image file as a \"header.\"</p>"},{"location":"ImagingFormats/#common-metadata-fields","title":"Common Metadata Fields","text":"<ul> <li>Image dimensions: Width, height, depth</li> <li>Voxel size: Spacing between voxels</li> <li>Origin: Location of the first voxel</li> <li>Orientation: Direction of x, y, and z axes</li> <li>Pixel depth: Bytes used to represent each voxel intensity</li> <li>Data type: Integer, floating-point, etc.</li> </ul> <p>add diagram showing a sample header with key metadata fields</p>"},{"location":"ImagingFormats/#pixel-data","title":"Pixel Data","text":"<p>The pixel data in a medical image file represents the actual image values, stored in a format specific to the image file.</p> <ul> <li>In fixed-size header formats, pixel data follows the header directly.</li> <li>In other formats, a marker or tag indicates the start of pixel data.</li> </ul>"},{"location":"ImagingFormats/#pixel-data-size","title":"Pixel Data Size","text":"\\[ \\text{Pixel Data Size} = \\text{Rows} \\times \\text{Columns} \\times \\text{Pixel Depth (Bytes)} \\times \\text{Number of Frames} \\]"},{"location":"ImagingFormats/#image-file-size","title":"Image File Size","text":"\\[ \\text{Image File Size} = \\text{Header Size} + \\text{Pixel Data Size} \\] Example Calculation: <p>For a DICOM image with the following parameters:</p> Parameter Value Rows 512 Columns 512 Pixel Depth 2 bytes (16-bit image) Number of Frames 32 (32 slices) \\[ \\text{Pixel Data Size} = 512 \\times 512 \\times 2 \\times 32 = 16,777,216 \\text{ bytes (or 16 MB)} \\] <p>Assuming the header size for this DICOM file is 1,024 bytes:</p> \\[ \\text{Image File Size} = 1,024 \\text{ bytes} + 16,777,216 \\text{ bytes} = 16,778,240 \\text{ bytes (or 16.01 MB)} \\]"},{"location":"ImagingFormats/#medical-image-file-formats","title":"Medical Image File Formats","text":""},{"location":"ImagingFormats/#categories-of-medical-image-formats","title":"Categories of Medical Image Formats","text":"<ol> <li> <p>Standardization Formats: Standardize images from diagnostic modalities.</p> <ul> <li>Example: DICOM</li> </ul> </li> <li> <p>Post-Processing Formats: Facilitate and strengthen post-processing analysis.</p> <ul> <li>Examples: Analyze, NIfTI, MINC</li> </ul> </li> </ol>"},{"location":"ImagingFormats/#configurations-for-storing-medical-images","title":"Configurations for Storing Medical Images","text":"<ul> <li> <p>Single File: Contains both metadata and image data, with metadata stored   at the beginning.</p> <ul> <li>Examples: DICOM, MINC, NIfTI</li> </ul> </li> <li> <p>Two Files: Metadata and image data stored separately.</p> <ul> <li>Example: Analyze (.hdr and .img)</li> </ul> </li> </ul> <p>add diagram comparing single file vs. two-file configurations</p>"},{"location":"ImagingFormats/DICOM/","title":"DICOM (Digital Imaging and Communications in Medicine)","text":"<p>TODO: add introduction to DICOM</p> <p>TODO: Add references if using images from online </p> <p></p> <p></p>"},{"location":"ImagingFormats/nifti/","title":"Nifti Format","text":"<p>https://nipy.org/nibabel/coordinate_systems.html https://nipy.org/nibabel/nifti_images.html NIFTI first poster: https://nifti.nimh.nih.gov/nifti-1/documentation/hbm_nifti_2004.pdf NIfTI structure diagram: https://nifti.nimh.nih.gov/nifti-1/documentation/nifti1diagrams_v2.pdf</p>"},{"location":"ImagingFormats/nifti/#introduction-to-nifti","title":"Introduction to NIfTI","text":"<p>The NIfTI (Neuroimaging Informatics Technology Initiative) format is a derivative of the ANALYZE format, which was originally developed for medical imaging.</p>"},{"location":"ImagingFormats/nifti/#why-nifti","title":"Why NIfTI?","text":"<p>Before NIfTI, medical imaging data was stored in a variety of formats,  including ANALYZE, MINC, and DICOM.</p> <ul> <li>ANALYZE<ul> <li>ANALYZE is a proprietary format developed by the Mayo Clinic.</li> <li>ANALYZE files are not widely supported and are often difficult to work with.</li> </ul> </li> <li>MINC<ul> <li>MINC (Medical Image NetCDF) is a newer format developed by the National  Institutes of Health (NIH).</li> <li>MINC files are widely supported and can be easily shared and distributed.</li> </ul> </li> <li>DICOM<ul> <li>DICOM (Digital Imaging and Communications in Medicine) is a standardized  format for medical imaging data.</li> <li>DICOM files are widely supported and can be easily shared and distributed.</li> </ul> </li> </ul>"},{"location":"ImagingFormats/nifti/#references","title":"References","text":"<ol> <li>Medical Image File Formats, Michele Larobina, 2014.</li> <li>https://brainder.org/2012/09/23/the-nifti-file-format/</li> <li>http://www.cognitive-antics.net/2014/03/04/nifti-plain-and-simple/</li> </ol>"},{"location":"Introduction/Challenges/","title":"Challenges in Medical Imaging Standardization","text":"<p>Standardizing medical imaging data for AI applications presents several challenges:</p> <ul> <li>Variability in Imaging Modalities: Different imaging techniques (e.g., MRI,   CT, PET) produce data with distinct characteristics, making uniform   standardization difficult.</li> <li>Inconsistent Metadata: Metadata formats vary across institutions and   devices, complicating the integration of diverse datasets.</li> <li>Resolution and Quality Differences: Variations in image resolution and   quality across sources can affect model training, as AI models are sensitive   to inconsistencies in input data.</li> <li>Interoperability Issues: Lack of standardized file formats and data   structures hinders interoperability between platforms and research tools.</li> <li>Data Annotation Challenges: Ensuring consistency in annotation across   large datasets is resource-intensive, requiring expert input and validation.</li> </ul>"},{"location":"Introduction/EthicalConsiderations/","title":"Ethical Considerations in AI-driven Cancer Research","text":"<p>Developing AI models for cancer research introduces important ethical considerations:</p> <ul> <li>Bias in Data Representation: Ensuring diverse representation in the dataset   is essential to prevent AI models from inheriting biases that could impact   underrepresented groups.</li> <li>Transparency and Explainability: AI models should be transparent and   interpretable, allowing clinicians to understand the basis of AI-driven   recommendations.</li> <li>Informed Consent: Patients\u2019 consent for the use of their medical images   in research must be clearly defined, especially when sharing data across   institutions.</li> <li>Accountability in Decision-Making: AI models used in clinical settings must   have clear accountability guidelines to manage potential errors and adverse   outcomes.</li> <li>Equitable Access to AI Innovations: The benefits of AI-driven advancements   should be accessible across various healthcare settings, including those   with limited resources.</li> </ul>"},{"location":"Introduction/EvaluationValidation/","title":"Evaluation and Validation Framework for AI Models","text":"<p>To ensure the reliability and accuracy of AI models trained on Med-ImageNet, a rigorous evaluation and validation framework is implemented:</p> <ul> <li>Benchmark Datasets: Provides standardized benchmark datasets within   Med-ImageNet to evaluate model performance across common metrics.</li> <li>Cross-validation Protocols: Supports cross-validation to verify model   robustness and prevent overfitting, ensuring that models generalize well   across different patient groups.</li> <li>Performance Metrics: Includes metrics for evaluating segmentation   accuracy, predictive power, and clinical relevance of AI models.</li> <li>Human-in-the-Loop Validation: Allows experts to review and validate AI   model outputs, ensuring that predictions are clinically meaningful.</li> <li>Continuous Model Improvement: Framework supports ongoing model   evaluation, with feedback loops to integrate new data and improve   performance over time.</li> </ul>"},{"location":"Introduction/ImageNet/","title":"ImageNet: Transforming Computer Vision","text":"<p>This content was written by an LLM</p> <p>This content was written by an LLM and requires a thorough review and editing to ensure accuracy and relevance before publishing.</p>"},{"location":"Introduction/ImageNet/#overview-what-is-imagenet","title":"Overview: What is ImageNet?","text":"<p>ImageNet is a massive visual database created for use in visual object recognition research. With over 14 million labeled images across thousands of categories, ImageNet serves as the backbone for many advancements in computer vision.</p> <p>TODO: add image/diagram of ImageNet categories and dataset structure</p>"},{"location":"Introduction/ImageNet/#the-impact-of-imagenet-on-computer-vision","title":"The Impact of ImageNet on Computer Vision","text":""},{"location":"Introduction/ImageNet/#a-benchmark-for-innovation","title":"A Benchmark for Innovation","text":"<p>ImageNet set a new standard for comparing and evaluating the performance of image recognition algorithms. By establishing a consistent benchmark, ImageNet fostered rapid advancements across the field of computer vision.</p>"},{"location":"Introduction/ImageNet/#the-imagenet-large-scale-visual-recognition-challenge-ilsvrc","title":"The ImageNet Large Scale Visual Recognition Challenge (ILSVRC)","text":"<p>The annual ILSVRC competition has been a major driver of progress. Each year, researchers compete to push the boundaries of image recognition, leading to breakthroughs that have revolutionized AI and deep learning. These innovations have gone beyond image recognition, impacting diverse fields in artificial intelligence and machine learning.</p> <p>TODO: add image/diagram of ILSVRC competition timeline and key milestones</p>"},{"location":"Introduction/ImageNet/#key-breakthroughs-powered-by-imagenet","title":"Key Breakthroughs Powered by ImageNet","text":""},{"location":"Introduction/ImageNet/#alexnet-the-catalyst-for-cnns-2012","title":"AlexNet: The Catalyst for CNNs (2012)","text":"<ul> <li>Achievement: First deep CNN to win ILSVRC, showcasing the potential of   convolutional neural networks (CNNs) for image classification.</li> <li>Impact: Paved the way for CNN architectures, proving their power on a   large-scale dataset.</li> </ul> <p>TODO: add image/diagram of AlexNet architecture</p>"},{"location":"Introduction/ImageNet/#vggnet-deep-networks-simplified-2014","title":"VGGNet: Deep Networks Simplified (2014)","text":"<ul> <li>Achievement: VGGNet demonstrated that increasing network depth can   improve performance.</li> <li>Key Feature: Simple and uniform architecture that added depth without   complexity.</li> </ul> <p>TODO: add image showing VGGNet depth and structure</p>"},{"location":"Introduction/ImageNet/#resnet-the-power-of-residual-learning-2015","title":"ResNet: The Power of Residual Learning (2015)","text":"<ul> <li>Achievement: Introduced the concept of residual connections, making it   possible to train ultra-deep networks.</li> <li>Impact: Enabled breakthroughs in deep learning by overcoming vanishing   gradient issues.</li> </ul> <p>TODO: add image/diagram of residual learning concept in ResNet</p>"},{"location":"Introduction/ImageNet/#yolo-you-only-look-once-real-time-object-detection-2016","title":"YOLO (You Only Look Once): Real-time Object Detection (2016)","text":"<ul> <li>Achievement: Developed a highly efficient real-time object detection   system that uses pre-trained ImageNet models.</li> <li>Impact: Revolutionized object detection, making it faster and more   accessible for applications.</li> </ul> <p>TODO: add image/diagram of YOLO\u2019s object detection in action</p>"},{"location":"Introduction/ImageNet/#mobilenet-lightweight-models-for-mobile-applications-2017","title":"MobileNet: Lightweight Models for Mobile Applications (2017)","text":"<ul> <li>Achievement: Optimized for mobile and embedded devices, leveraging   ImageNet for efficient image recognition.</li> <li>Impact: Brought advanced image recognition to mobile applications,   enabling real-time processing on low-power devices.</li> </ul> <p>TODO: add image/diagram of MobileNet architecture and mobile applications</p>"},{"location":"Introduction/ImageNet/#conclusion-the-lasting-influence-of-imagenet","title":"Conclusion: The Lasting Influence of ImageNet","text":"<p>ImageNet has been foundational in developing computer vision and deep learning technologies. Its role in creating a common benchmark and encouraging innovation through the ILSVRC has led to innovations that extend far beyond image recognition, impacting diverse areas of AI and machine learning. The dataset's influence will continue to shape the landscape of computer vision for years to come.</p> <p>TODO: add image summarizing the impact of ImageNet on AI and machine learning fields</p>"},{"location":"Introduction/ImageNet/#extra-resources","title":"Extra Resources","text":"<ul> <li>ImageNet: A Large-Scale Hierarchical Image Database</li> <li>ImageNet Large Scale Visual Recognition Challenge (ILSVRC)</li> <li>ImageNet: A Large Scale Hierarchical Image Database</li> <li>ImageNet: A Large Scale Hierarchical Image Database</li> </ul>"},{"location":"Introduction/MedImageTools_Specification/","title":"Technical Specifications of MedImage-Tools","text":"<p>MedImage-Tools is designed to streamline the standardization and processing of medical imaging data for AI applications.</p> <p>Key specifications include:</p> <ul> <li>Automated Data Standardization: Tools to automatically convert diverse   imaging formats and metadata into a unified format compatible with Med-ImageNet.</li> <li>Annotation Interface: A user-friendly interface for precise image   annotation, allowing researchers to label and segment data consistently.</li> <li>Scalability: Optimized to handle large datasets efficiently, with support   for batch processing and parallel computations.</li> <li>Customizable Pipelines: Provides modular pipeline components that can be   tailored to specific research needs, including options for preprocessing,   augmentation, and validation.</li> <li>Integration with FAIR Principles: Designed to ensure data is Findable,   Accessible, Interoperable, and Reusable, supporting easy sharing and   collaboration across research teams.</li> </ul>"},{"location":"Overview/","title":"Med-ImageNet: Open-source Medical Imaging Data Curation for Large-scale AI","text":"<p>This documentation is a work in progress.</p> <p>The Med-ImageNet project is currently under development, and this documentation is being updated to reflect the latest progress and features.</p> <p>Please check back for updates and additional content.</p>"},{"location":"Overview/#project-description","title":"Project Description","text":"<p>Med-ImageNet is a transformative framework aiming to facilitate access to standardized medical imaging dataset for cancer research and clinical AI applications.</p>"},{"location":"Overview/#key-information","title":"Key Information","text":"<ul> <li> <p>\ud83d\uddbc Project Title: Med-ImageNet</p> </li> <li> <p>\ud83e\uddec Objective:  Standardize and curate oncology imaging data to support AI-driven cancer research, focusing on auto-segmentation, treatment planning, and monitoring.</p> </li> <li> <p>\ud83c\udf10 Significance: Tackles the lack of standardized healthcare imaging data, making data FAIR (Findable, Accessible, Interoperable, and Reusable) for machine learning applications.</p> </li> <li> <p>\ud83d\udcbb Innovations: Develops MedImage-Tools for data standardization, providing a comprehensive, open-source dataset for cancer imaging.</p> </li> <li> <p>\ud83c\udf0d Equity and Inclusion: Prioritizes data inclusivity by representing diverse populations in cancer research.</p> </li> <li> <p>\ud83e\udd1d Collaboration: Engages with the AI and medical communities to foster collaboration and innovation in cancer research.</p> </li> </ul>"},{"location":"Overview/AI_Applications/","title":"AI Applications in Medical Imaging","text":""},{"location":"Overview/AI_Applications/#auto-segmentation-and-labeling","title":"Auto-Segmentation and Labeling","text":"<ul> <li>Automated Segmentation: Utilize Med-ImageNet data to train models that   can perform automatic segmentation of tumors and relevant structures within   medical images, reducing the time and effort required for manual labeling.</li> <li>Annotation Accuracy: Ensure that auto-segmentation tools produce precise   and clinically relevant boundaries, enhancing the quality of downstream AI   applications.</li> <li>Model Validation: Implement rigorous validation processes to assess the   reliability and accuracy of segmentation models, making them suitable for   clinical and research applications.</li> </ul>"},{"location":"Overview/AI_Applications/#treatment-planning-and-monitoring","title":"Treatment Planning and Monitoring","text":"<ul> <li>AI-driven Planning: Enable AI models trained on Med-ImageNet data to   support personalized treatment planning by identifying key anatomical   features and potential risks.</li> <li>Patient Monitoring: Leverage imaging data to monitor treatment progress   and detect early signs of relapse or treatment response, enabling timely   interventions.</li> <li>Outcome Prediction: Develop models that use imaging features to predict   patient outcomes, supporting oncologists in making data-driven treatment   decisions.</li> </ul>"},{"location":"Overview/AI_Applications/#open-ml-challenge-for-head-and-neck-cancer","title":"Open ML Challenge for Head and Neck Cancer","text":"<ul> <li>Challenge Objective: Host a public machine learning challenge focused on   head and neck cancer segmentation and analysis to encourage innovation and   improve model accuracy.</li> <li>Data Access and Submission: Provide participants with access to   anonymized Med-ImageNet data for training and testing, with submissions   evaluated on segmentation accuracy and clinical relevance.</li> <li>Advancing Model Generalizability: Use the challenge to identify models   that generalize well across diverse data, with high potential for clinical   implementation.</li> </ul>"},{"location":"Overview/Methodology/","title":"Methodology","text":""},{"location":"Overview/Methodology/#data-collection-and-curation-process","title":"Data Collection and Curation Process","text":"<ul> <li>Source Identification: Identify and collect diverse imaging data from   trusted sources, ensuring a broad representation of cancer types and   demographic groups.</li> <li>Data Cleaning and Preprocessing: Implement rigorous data cleaning to   remove inconsistencies and prepare images for standardized processing.</li> <li>Annotation Standards: Apply consistent annotation protocols across all   datasets, with expert validation to ensure quality and reliability.</li> </ul>"},{"location":"Overview/Methodology/#medimage-tools-development","title":"MedImage-Tools Development","text":"<ul> <li>Tool Design: Develop tools that streamline the standardization process,   allowing researchers to easily convert raw imaging data into a format   compatible with Med-ImageNet standards.</li> <li>Automation and Scalability: Design MedImage-Tools with automation   capabilities to handle large-scale data efficiently, ensuring the dataset   remains up-to-date with minimal manual intervention.</li> </ul>"},{"location":"Overview/Methodology/#integration-of-fair-principles","title":"Integration of FAIR Principles","text":"<ul> <li>Findability: Ensure that all dataset components are tagged and indexed   for easy discoverability within the research community.</li> <li>Accessibility: Provide an open-access platform where researchers can   securely access and download data for AI model training and validation.</li> <li>Interoperability: Structure data in widely accepted formats, facilitating   cross-platform compatibility and collaboration.</li> <li>Reusability: Design data with future adaptability in mind, enabling its   application across a variety of oncology research initiatives.</li> </ul>"},{"location":"Overview/ProjectJustification/","title":"Project Justification","text":"<p>Abstract</p> <p>Med-ImageNet addresses a major gap: a standardized framework for accessible, large-scale imaging datasets specifically for AI in oncology.</p> <p>Existing imaging datasets are fragmented, varying widely in quality, format, and annotation, which complicates AI model training and reliability.</p>"},{"location":"Overview/ProjectJustification/#current-gaps-in-medical-imaging-data","title":"Current Gaps in Medical Imaging Data","text":"<ul> <li> <p>Data Silos: Medical imaging data is often siloed within institutions,   limiting external access.</p> </li> <li> <p>Annotation Issues: Datasets lack detailed annotation required   for advanced AI tasks like auto-segmentation and treatment planning.</p> </li> <li> <p>Curation and Accessibility: Inadequate curation and   accessibility hinder the application of machine learning for accurate cancer   diagnosis and treatment.</p> </li> </ul>"},{"location":"Overview/ProjectJustification/#need-for-standardization-in-ai-research","title":"Need for Standardization in AI Research","text":"<ul> <li> <p>Consistency Across Datasets: Standardization is essential for   ensuring that AI models can perform reliably across diverse datasets. Without   consistent data, models risk overfitting to specific datasets, limiting their   broader applicability.</p> </li> <li> <p>Global Collaboration: A standardized framework   allows researchers worldwide to access uniform data, enhancing collaboration   and accelerating advancements in cancer treatment and AI development.</p> </li> </ul>"},{"location":"Overview/ProjectJustification/#impact-on-cancer-research-and-treatment","title":"Impact on Cancer Research and Treatment","text":"<ul> <li> <p>Enhanced Predictive Power: Access to a large,   standardized dataset boosts the predictive accuracy of AI models, helping   clinicians make better-informed treatment decisions.</p> </li> <li> <p>Improved Patient Outcomes: With more reliable AI models,   treatment planning can be personalized, leading to better outcomes and more   efficient resource allocation in oncology care.</p> </li> </ul>"},{"location":"Overview/ResearchObjectivesAims/","title":"Research Objectives and Aims","text":""},{"location":"Overview/ResearchObjectivesAims/#primary-goal","title":"Primary Goal","text":"<p>Develop an open-access, standardized medical imaging dataset specifically curated for cancer research and AI applications in oncology.</p>"},{"location":"Overview/ResearchObjectivesAims/#specific-objectives","title":"Specific Objectives","text":"<ol> <li> <p>Data Curation and Annotation:</p> <p>Establish robust pipelines for data collection, curation, and high-quality annotation to support machine learning tasks in cancer diagnosis and treatment.</p> </li> <li> <p>Med-ImageTools Development:</p> <p>Create a suite of tools to streamline data standardization, facilitating widespread adoption and usability of the Med-ImageNet dataset.</p> </li> <li> <p>AI Model Support:</p> <p>Enable training and validation of AI models for applications like tumor segmentation, treatment planning, and outcome prediction.</p> </li> </ol>"},{"location":"Overview/ResearchObjectivesAims/#long-term-vision","title":"Long-term Vision","text":"<p>Establish Med-ImageNet as the gold standard resource for AI-driven cancer research, encouraging global collaboration and advancing personalized oncology care.</p>"},{"location":"devnotes/","title":"A lot of these devnotes are just braindumps","text":"<p>As they become more coherent, they should be moved to issues / PRs / blog posts within the docs.</p>"},{"location":"devnotes/Med-ImageDB/","title":"Med-ImageDB (Medical Image Database)","text":""},{"location":"devnotes/Med-ImageDB/#description","title":"Description","text":"<p>Med-ImageDB aims to provide a index of sources that provide access to medical images, which can then be used for training and testing machine learning models.</p> <p>The database contains images of various modalities.</p>"},{"location":"devnotes/Med-ImageDB/#sources","title":"Sources","text":"<ul> <li> <p>ziyangwang007/Awesome-Medical-Image-Segmentation-Dataset</p> </li> <li> <p>adalca/medical-datasets</p> </li> <li> <p>aylward.org/open-access-medical-image-repositories</p> </li> <li> <p>m-aryayi/Medical-Imaging-Datasets</p> </li> </ul> <p>This one is actually very thorough and has 600+ links to publications and datasets.</p> <ul> <li>openmedlab/Awesome-Medical-Dataset</li> </ul> <p>Also very thorough and organized.</p>"},{"location":"devnotes/UserStoriesAndArchitecture/","title":"User Stories","text":""},{"location":"devnotes/UserStoriesAndArchitecture/#context","title":"Context","text":"<p>Med-ImageNet is a framework for accessing, retrieving, and preprocessing medical imaging datasets from public and user-provided sources.  It bridges raw imaging data and downstream AI/ML workflows with standardized tools for querying, loading, and visualizing medical images.</p>"},{"location":"devnotes/UserStoriesAndArchitecture/#user-pain-points","title":"User Pain Points","text":"<p>Medical imaging researchers and data scientists face a variety of challenges when  working with large-scale datasets and building reproducible pipelines. The following  pain points illustrate the key barriers that Med-ImageNet aims to address:</p> <ul> <li> <p>Fragmented Access to Public Datasets: Public repositories like TCIA are rich    resources, but their interfaces can be inconsistent, poorly documented, or    difficult to access programmatically. This slows down research and introduces    manual steps in data acquisition workflows.</p> </li> <li> <p>Limited Metadata Querying Capabilities: Finding relevant datasets based on    imaging modality, disease type, or annotation availability is often difficult.    Users typically need to manually inspect metadata or download large index files    to perform even basic filtering tasks.</p> </li> <li> <p>Inconsistent Data Formats and Standards: Imaging data is often stored in   DICOM format, which is supposed to be a standard but can vary in practice.   There are many tools online that make strict assumptions and are not always   robust with the massive variation in dicom data.   Moreover, researchers spend a significant amount of time converting data to   a common format (NIfTI, HDF5) for machine learning workflows.</p> </li> <li> <p>Lack of Reusable Preprocessing Pipelines: There is no common framework to    perform tasks such as intensity normalization, resampling, or segmentation.    Researchers often implement these steps manually, leading to duplicated effort    and difficulty in reproducing results.</p> </li> <li> <p>Poor Visualization Tooling for Medical Images: Many researchers lack tools to    interactively explore 2D/3D medical images with overlays of masks or annotations.    Visualization is often limited to static plots or external viewers with steep    learning curves.</p> </li> <li> <p>Scattered Tooling Ecosystem: Tools for downloading, preprocessing, visualizing,    and analyzing medical imaging data are scattered across different languages,    formats, and libraries. This leads to brittle pipelines and high onboarding time    for new users.</p> </li> </ul> <p>Med-ImageNet aims to eliminate these bottlenecks by providing a consistent, unified  Python interface for querying datasets, preprocessing images, and preparing data for  machine learning workflows.</p>"},{"location":"devnotes/UserStoriesAndArchitecture/#user-groups","title":"User Groups","text":"<p>These groups represent the primary users of Med-ImageNet and their respective needs. For the most part, these groups are not mutually exclusive and may overlap in practice.</p> <p>For example, since a primary feature of Med-ImageNet is to provide access to medical imaging datasets for downstream work, both ML Engineers and Bioinformaticians will follow a similar workflow of data access and preprocessing before diverging into their respective tasks of model training and feature analysis.</p>"},{"location":"devnotes/UserStoriesAndArchitecture/#machine-learning-engineers-data-scientists-mlds","title":"Machine Learning Engineers &amp; Data Scientists (ML/DS)","text":"<ul> <li>Develop and optimize deep learning models for medical imaging</li> <li>Create and validate AI models for diagnosis and prognosis</li> <li>Need efficient data loading and preprocessing pipelines</li> <li>Require standardized evaluation metrics and benchmarks</li> </ul>"},{"location":"devnotes/UserStoriesAndArchitecture/#computational-biologists-bioinformaticians-cbb","title":"Computational Biologists &amp; Bioinformaticians (CB/B)","text":"<ul> <li>Analyze radiomic features and multi-modal medical datasets</li> <li>Integrate imaging data with genomic and clinical information</li> <li>Perform statistical analysis and hypothesis testing</li> <li>Extract quantitative features from medical images</li> </ul>"},{"location":"devnotes/UserStoriesAndArchitecture/#software-engineers-devops-swedevops-stretch-goal","title":"Software Engineers &amp; DevOps (SWE/DevOps) [Stretch Goal]","text":"<ul> <li>Deploy and maintain Med-ImageNet in clinical or research environments</li> <li>Ensure system reliability and performance</li> <li>Implement security and access control measures</li> <li>Manage data storage and retrieval infrastructure</li> </ul>"},{"location":"devnotes/UserStoriesAndArchitecture/#user-needs","title":"User Needs","text":"<p>As a (ML/DS, CB/B), I would want to be able to</p> <ul> <li>know what access I have to publicly available datasets<ul> <li>Browse available datasets with clear documentation</li> <li>Understand dataset characteristics and limitations</li> <li>Access dataset statistics and quality metrics</li> </ul> </li> <li>know how I can access and download these datasets<ul> <li>Simple authentication and authorization process</li> <li>Clear documentation on data usage rights</li> <li>Efficient download mechanisms for large datasets</li> </ul> </li> <li>query publicly available datasets for specific data<ul> <li>Search by modality, disease, patient characteristics</li> <li>Filter by image quality and annotation availability</li> <li>Access metadata programmatically</li> </ul> </li> </ul> <p>As a (ML/DS, CB/B), I want a set of tools that can help me</p> <ul> <li>preprocess and clean medical imaging data<ul> <li>converting to <code>SimpleITK.Image</code> objects </li> <li>indexing and querying metadata</li> </ul> </li> <li>visualize and explore medical imaging data<ul> <li>2D/3D visualization</li> <li>interactive exploration</li> </ul> </li> </ul> <p>As a (ML/DS, CB/B), I want a single Python API that can</p> <ul> <li>load and access (potentially large) medical imaging datasets</li> <li>provide a consistent interface for data access and preprocessing</li> <li>integrate with popular deep learning frameworks (e.g. PyTorch, TensorFlow)</li> </ul>"},{"location":"devnotes/UserStoriesAndArchitecture/#user-personas-flow","title":"User Personas Flow","text":"<pre><code>graph TD\n    ML[Machine Learning Engineer&lt;br&gt;&amp; Data Scientist] --&gt;|Develops| AI[AI Models]\n    ML --&gt;|Uses| Data[Data Pipeline]\n\n    CB[Computational Biologist&lt;br&gt;&amp; Bioinformatician] --&gt;|Analyzes| Features[Radiomic Features]\n    CB --&gt;|Integrates| Multi[Multi-modal Data]\n\n    SWE[Software Engineer&lt;br&gt;&amp; DevOps] --&gt;|Maintains| Infra[Infrastructure]\n    SWE --&gt;|Ensures| Security[Security &amp; Access]\n\n    Data --&gt;|Feeds| AI\n    Data --&gt;|Provides| Features\n    Infra --&gt;|Supports| Data</code></pre>"},{"location":"devnotes/UserStoriesAndArchitecture/#architecture","title":"Architecture","text":"<p>The architecture of Med-ImageNet is designed to provide seamless access to public medical imaging datasets, while also allowing users to bring their own data for analysis and processing. The system consists of three major components:</p>"},{"location":"devnotes/UserStoriesAndArchitecture/#components","title":"Components","text":"<ol> <li> <p>Public Medical Imaging Databases</p> <ul> <li>These databases host publicly available medical imaging datasets, including radiological images (CT, MRI, PET) and associated metadata such as patient demographics, pathology reports, and annotations</li> <li>Example: TCIA (The Cancer Imaging Archive) provides a repository of de-identified medical images for research</li> <li>Additionally, some databases may have <code>Annotation</code> data that can be used for training and validation of AI models such as <code>DICOM-SEG</code> or <code>DICOM-RTSTRUCT</code></li> </ul> </li> <li> <p>Med-ImageDB (Data Indexing &amp; Access Layer)</p> <ul> <li>A Python package that acts as an intermediary between users and multiple public databases</li> <li>Indexes datasets from sources like TCIA, making them searchable and accessible through a standardized API</li> <li>Supports metadata queries to filter datasets by modality, annotation availability, ROI Name, etc.</li> <li>Handles (maybe) authentication, and efficient downloading of large datasets</li> </ul> </li> <li> <p>Med-ImageTools (Processing &amp; Preprocessing Layer)</p> <ul> <li>A Python package that enables seamless data processing for both indexed datasets from Med-ImageDB and user-provided imaging data</li> <li>Supports format conversion (e.g., SimpleITK.Image, numpy.ndarray)</li> <li>Implements preprocessing functions such as intensity normalization, resampling, and segmentation</li> <li>Provides visualization tools for 2D and 3D exploration of medical images</li> <li>Integrates with deep learning frameworks (e.g., PyTorch, TensorFlow) for model training and inference</li> </ul> </li> </ol>"},{"location":"devnotes/UserStoriesAndArchitecture/#user-data-flow-interaction","title":"User Data Flow &amp; Interaction","text":"<p>There are two main pathways for users:</p> <ol> <li> <p>Public Dataset Access &amp; Processing</p> <ul> <li>Users query Med-ImageDB to find relevant datasets from public sources</li> <li>The system retrieves dataset metadata from sources like TCIA</li> <li>Users download datasets through Med-ImageDB</li> <li>The raw imaging data is passed to Med-ImageTools for preprocessing and analysis</li> </ul> </li> <li> <p>User-Provided Data Processing</p> <ul> <li>Users can bring their own imaging data directly into Med-ImageTools, bypassing Med-ImageDB</li> <li>The same preprocessing and analysis tools are available for both public and    user-provided datasets, they will have to process themselves.</li> <li>This allows for flexibility in research workflows, integrating proprietary or experimental datasets</li> </ul> </li> </ol>"},{"location":"devnotes/UserStoriesAndArchitecture/#architecture-diagram","title":"Architecture Diagram","text":"<pre><code>flowchart TD\n subgraph External[\"External\"]\n        TCIA[\"TCIA: The Cancer Imaging Archive\"]\n        DB1[\"NMDID: New Mexico Decedent Image Database\"]\n        DB2[\"PlaceholderDB2\"]\n        UserData[\"User-Provided Data\"]\n  end\n subgraph s1[\"Med-ImageDB\"]\n        Indexing[\"Dataset Indexing\"]\n        Query[\"Search &amp; Query API\"]\n        Access[\"Data Access &amp; Retrieval\"]\n  end\n subgraph s2[\"Med-ImageTools\"]\n        Preprocess[\"Preprocessing\"]\n        Visualize[\"Visualization\"]\n  end\n subgraph s3[\"Med-ImageNet\"]\n        s1\n        s2\n  end\n    Users[\"ML Engineers / Bioinformaticians\"] -- Search, Retrieve --&gt; Query\n    Users -- Directly Upload Data --&gt; UserData\n    TCIA -- Indexes --&gt; Indexing\n    DB1 -- Indexes --&gt; Indexing\n    DB2 -- Indexes --&gt; Indexing\n    UserData -- \"Bypass Med-ImageDB\" --&gt; Preprocess\n    Indexing -- Provides Searchable Data --&gt; Query\n    Query -- Retrieves Data --&gt; Access\n    Access -- Provides Raw Imaging Data (DICOM) --&gt; Preprocess\n    Preprocess -- \"Processes AI-Ready Data\" --&gt; AI-Ready[\"Deep Learning Integration\"]\n    Access -- \"Provides AI-Ready Data (NIFTI)\" --&gt; AI-Ready\n    Preprocess -- Processed Images --&gt; Visualize</code></pre>"},{"location":"devnotes/UserStoriesAndArchitecture/#technical-scope","title":"Technical Scope","text":"<p>Defining granular technical requirements for each componenet</p> <ol> <li> <p>Med-ImageDB</p> <ul> <li>Input: TCIA API / List of public datasets</li> <li>Output: Med-ImageNet index</li> </ul> </li> <li> <p>Med-ImageTools</p> <ul> <li>Input: Raw Imaging Data / DICOM (CT, MR, PET, RTDOSE, RTSTRUCT, SEG)</li> <li>Output: AI-Ready Data / NIfTI, HDF5</li> </ul> </li> </ol>"},{"location":"devnotes/UserStoriesAndArchitecture/#glossary","title":"Glossary","text":"<p>AI-Ready Imaging data that has been preprocessed to meet the input requirements of machine learning pipelines.</p> <ul> <li>This typically includes format conversion (e.g.,     DICOM to NIfTI) and possibly some transformations. </li> <li>Importantly, there will should be some metadata to accompany the data     that adds context to the data, including fields from the DICOM file,     computed information (size, shape, etc.), possible references to     other series or studies, and any annotations that may be available.</li> </ul> <p>Annotation Supplementary data associated with medical images that describe structures, regions of interest (ROIs), or diagnoses. Examples include segmentations (DICOM-SEG), radiotherapy structures (DICOM-RTSTRUCT), or bounding boxes.</p> <p>DICOM (Digital Imaging and Communications in Medicine) A standard format for storing and transmitting medical images and metadata. DICOM files are often used in clinical imaging and contain detailed metadata about the patient, acquisition parameters, and study information.</p> <p>DICOM-SEG A standardized DICOM object for storing segmentation masks. Each mask encodes a region of interest (e.g., tumor) as a labelmap aligned to the original image.</p> <p>DICOM-RTSTRUCT A type of DICOM object used in radiotherapy to define contours of anatomical structures. These are often used for planning radiation therapy and may contain multiple labeled ROIs.</p> <p>Indexing The process of collecting and organizing metadata about imaging datasets to enable fast search, filtering, and access. Med-ImageDB performs indexing from public sources like TCIA.</p> <p>Metadata Information about an image or dataset, such as patient age, modality, scan date, disease site, and annotation availability. Metadata is critical for filtering, querying, and dataset selection.</p> <p>Modality The imaging technique used to acquire the data, such as CT (Computed Tomography), MRI (Magnetic Resonance Imaging), or PET (Positron Emission Tomography).</p> <p>NIfTI (Neuroimaging Informatics Technology Initiative) A common file format (.nii or .nii.gz) used for storing medical imaging data in research, particularly in neuroimaging and machine learning applications.</p> <p>Preprocessing The set of operations applied to raw imaging data to prepare it for analysis or modeling. This may include resampling, normalization, cropping, masking, or alignment to a reference space.</p> <p>Radiomics The extraction of quantitative features from medical images, often used to describe shape, texture, and intensity patterns. Radiomics enables non-invasive characterization of tumors and other structures.</p> <p>Region of Interest (ROI) A specific area within an image that is annotated for analysis, such as a tumor, organ, or anatomical region. ROIs are usually defined using masks or contours.</p> <p>Resampling The process of changing the spatial resolution or voxel spacing of an image to ensure consistency across samples. This is important for aligning data from different scanners or modalities.</p> <p>SimpleITK A simplified interface to the Insight Segmentation and Registration Toolkit (ITK), used for reading, writing, and processing medical images in Python.</p> <p>TCIA (The Cancer Imaging Archive) A publicly accessible repository of de-identified medical images organized into collections, often with accompanying clinical and genomic data. Med-ImageNet uses TCIA as a primary public data source.</p> <p>Visualization Tools or processes used to explore and inspect medical images in 2D or 3D. Visualization may include overlaying masks, plotting slices, or interactive navigation across image volumes.</p>"},{"location":"devnotes/Inspiration/","title":"Inspiration","text":"<p>A collection of resources that relate to this project</p> <ul> <li>Different projects that can be used as inspiration for their code organization, user interface, etc.</li> </ul>"},{"location":"devnotes/Inspiration/amid/","title":"neuro-ml/amid (Awesome Medical Imaging Datasets)","text":"<p>Github repository: neuro-ml/amid</p> <p>Really interesting way they have set up a easy-to-use interface for loading datasets.</p> <p>Pros:</p> <ul> <li>similar user API for each dataset</li> <li>very nice code organization</li> <li>neat code quality</li> </ul> <p>Cons:</p> <ul> <li>Manually created code for each dataset</li> <li>Not very scalable</li> <li>Requires users to download and organize the datasets themselves.</li> </ul>"}]}